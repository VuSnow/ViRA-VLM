self_attention:
  num_heads: 8
  ffn_multiplier: 4
  dropout: 0.1
  add_positional: true

cross_attention:
  num_heads: 8
  dropout: 0.1
  ffn_multiplier: 4
  add_positional: true

eva_vision_model:
  name: "eva02_base_patch14_448.mim_in22k_ft_in22k_in1k"
  pretrained: true
  select_layer: -2
  select_feature: "patch"
  image_size: 448
  mean: [0.48145466, 0.4578275, 0.40821073]
  std: [0.26862954, 0.26130258, 0.27577711]
  requires_grad: false

deep_fusion:
  n_layers: 2
  n_heads: 8
  ffn_multiplier: 4
  dropout: 0.1
  add_positional: true
  max_seq_len: 2048
  
tie_word_embeddings: true
language_model:
  name: "SeaLLMs/SeaLLMs-v3-1.5B"
  requires_grad: true
  inject_layers: 2
  num_heads: 8
  ffn_multiplier: 4
  dropout: 0.1
  add_positional: true

use_lora: true
lora:
  r: 8
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
    - "gate_proj"
    - "down_proj"
    - "up_proj"
    - "lm_head"

training:
  output_dir: "/home/user05/dungvm/stage1_description/outputs/stage1"
  overwrite_output_dir: true
  eval_strategy: "epoch"
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 16
  eval_accumulation_steps: 16
  learning_rate: 2e-5
  weight_decay: 0.01
  optim: "adamw_torch"
  num_train_epochs: 5
  logging_dir: "/home/user05/dungvm/stage1_description/logs/stage1"
  logging_strategy: "steps"
  logging_first_step: true
  logging_steps: 100
  save_strategy: "epoch"
  save_total_limit: 1
  save_safetensors: true
  seed: 42
  bf16: false
  fp16: false
  dataloader_num_workers: 8
  disable_tqdm: false
  load_best_model_at_end: true
  metric_for_best_model: "rougeL"
  greater_is_better: true
  batch_eval_metrics: true
  report_to: "tensorboard"
  early_stopping: true
  early_stopping_patience: 3
  remove_unused_columns: false
  max_grad_norm: 1.0
  warmup_ratio: 0.05
  
